INFO:root:{'memory': None, 'steps': [('vec', TfidfVectorizer(max_features=5000, ngram_range=(1, 3),
                tokenizer=<function word_tokenize at 0x7fba51f49430>)), ('cls', RandomForestClassifier(random_state=0))], 'verbose': False, 'vec': TfidfVectorizer(max_features=5000, ngram_range=(1, 3),
                tokenizer=<function word_tokenize at 0x7fba51f49430>), 'cls': RandomForestClassifier(random_state=0), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.float64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1, 'vec__ngram_range': (1, 3), 'vec__norm': 'l2', 'vec__preprocessor': None, 'vec__smooth_idf': True, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__sublinear_tf': False, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x7fba51f49430>, 'vec__use_idf': True, 'vec__vocabulary': None, 'cls__bootstrap': True, 'cls__ccp_alpha': 0.0, 'cls__class_weight': None, 'cls__criterion': 'gini', 'cls__max_depth': None, 'cls__max_features': 'auto', 'cls__max_leaf_nodes': None, 'cls__max_samples': None, 'cls__min_impurity_decrease': 0.0, 'cls__min_samples_leaf': 1, 'cls__min_samples_split': 2, 'cls__min_weight_fraction_leaf': 0.0, 'cls__n_estimators': 100, 'cls__n_jobs': None, 'cls__oob_score': False, 'cls__random_state': 0, 'cls__verbose': 0, 'cls__warm_start': False}
INFO:root:{'memory': None, 'steps': [('vec', TfidfVectorizer(max_features=5000, ngram_range=(1, 3),
                tokenizer=<function word_tokenize at 0x7fba51f49430>)), ('cls', RandomForestClassifier(ccp_alpha=0.01, random_state=0))], 'verbose': False, 'vec': TfidfVectorizer(max_features=5000, ngram_range=(1, 3),
                tokenizer=<function word_tokenize at 0x7fba51f49430>), 'cls': RandomForestClassifier(ccp_alpha=0.01, random_state=0), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.float64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1, 'vec__ngram_range': (1, 3), 'vec__norm': 'l2', 'vec__preprocessor': None, 'vec__smooth_idf': True, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__sublinear_tf': False, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x7fba51f49430>, 'vec__use_idf': True, 'vec__vocabulary': None, 'cls__bootstrap': True, 'cls__ccp_alpha': 0.01, 'cls__class_weight': None, 'cls__criterion': 'gini', 'cls__max_depth': None, 'cls__max_features': 'auto', 'cls__max_leaf_nodes': None, 'cls__max_samples': None, 'cls__min_impurity_decrease': 0.0, 'cls__min_samples_leaf': 1, 'cls__min_samples_split': 2, 'cls__min_weight_fraction_leaf': 0.0, 'cls__n_estimators': 100, 'cls__n_jobs': None, 'cls__oob_score': False, 'cls__random_state': 0, 'cls__verbose': 0, 'cls__warm_start': False}
INFO:root:{'memory': None, 'steps': [('vec', TfidfVectorizer(max_features=5000, ngram_range=(1, 3),
                tokenizer=<function word_tokenize at 0x7fba51f49430>)), ('cls', RandomForestClassifier(ccp_alpha=0.001, random_state=0))], 'verbose': False, 'vec': TfidfVectorizer(max_features=5000, ngram_range=(1, 3),
                tokenizer=<function word_tokenize at 0x7fba51f49430>), 'cls': RandomForestClassifier(ccp_alpha=0.001, random_state=0), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.float64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1, 'vec__ngram_range': (1, 3), 'vec__norm': 'l2', 'vec__preprocessor': None, 'vec__smooth_idf': True, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__sublinear_tf': False, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x7fba51f49430>, 'vec__use_idf': True, 'vec__vocabulary': None, 'cls__bootstrap': True, 'cls__ccp_alpha': 0.001, 'cls__class_weight': None, 'cls__criterion': 'gini', 'cls__max_depth': None, 'cls__max_features': 'auto', 'cls__max_leaf_nodes': None, 'cls__max_samples': None, 'cls__min_impurity_decrease': 0.0, 'cls__min_samples_leaf': 1, 'cls__min_samples_split': 2, 'cls__min_weight_fraction_leaf': 0.0, 'cls__n_estimators': 100, 'cls__n_jobs': None, 'cls__oob_score': False, 'cls__random_state': 0, 'cls__verbose': 0, 'cls__warm_start': False}
INFO:root:{'memory': None, 'steps': [('vec', TfidfVectorizer(max_features=5000, ngram_range=(1, 3),
                tokenizer=<function word_tokenize at 0x7fba51f49430>)), ('cls', RandomForestClassifier(ccp_alpha=0.0001, random_state=0))], 'verbose': False, 'vec': TfidfVectorizer(max_features=5000, ngram_range=(1, 3),
                tokenizer=<function word_tokenize at 0x7fba51f49430>), 'cls': RandomForestClassifier(ccp_alpha=0.0001, random_state=0), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.float64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1, 'vec__ngram_range': (1, 3), 'vec__norm': 'l2', 'vec__preprocessor': None, 'vec__smooth_idf': True, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__sublinear_tf': False, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x7fba51f49430>, 'vec__use_idf': True, 'vec__vocabulary': None, 'cls__bootstrap': True, 'cls__ccp_alpha': 0.0001, 'cls__class_weight': None, 'cls__criterion': 'gini', 'cls__max_depth': None, 'cls__max_features': 'auto', 'cls__max_leaf_nodes': None, 'cls__max_samples': None, 'cls__min_impurity_decrease': 0.0, 'cls__min_samples_leaf': 1, 'cls__min_samples_split': 2, 'cls__min_weight_fraction_leaf': 0.0, 'cls__n_estimators': 100, 'cls__n_jobs': None, 'cls__oob_score': False, 'cls__random_state': 0, 'cls__verbose': 0, 'cls__warm_start': False}
