INFO:root:linear kernel: C = 10  f1 = 0.8265440645640952
INFO:root:{'memory': None, 'steps': [('vec', TfidfVectorizer(tokenizer=<function word_tokenize at 0x12689cd30>)), ('cls', SVC(C=1000, kernel='linear'))], 'verbose': False, 'vec': TfidfVectorizer(tokenizer=<function word_tokenize at 0x12689cd30>), 'cls': SVC(C=1000, kernel='linear'), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.float64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1, 'vec__ngram_range': (1, 1), 'vec__norm': 'l2', 'vec__preprocessor': None, 'vec__smooth_idf': True, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__sublinear_tf': False, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x12689cd30>, 'vec__use_idf': True, 'vec__vocabulary': None, 'cls__C': 1000, 'cls__break_ties': False, 'cls__cache_size': 200, 'cls__class_weight': None, 'cls__coef0': 0.0, 'cls__decision_function_shape': 'ovr', 'cls__degree': 3, 'cls__gamma': 'scale', 'cls__kernel': 'linear', 'cls__max_iter': -1, 'cls__probability': False, 'cls__random_state': None, 'cls__shrinking': True, 'cls__tol': 0.001, 'cls__verbose': False}
INFO:root:linear kernel: C = 100  f1 = 0.8389504411915761
INFO:root:{'memory': None, 'steps': [('vec', TfidfVectorizer(tokenizer=<function word_tokenize at 0x12339cd30>)), ('cls', LinearSVC(C=1000))], 'verbose': False, 'vec': TfidfVectorizer(tokenizer=<function word_tokenize at 0x12339cd30>), 'cls': LinearSVC(C=1000), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.float64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1, 'vec__ngram_range': (1, 1), 'vec__norm': 'l2', 'vec__preprocessor': None, 'vec__smooth_idf': True, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__sublinear_tf': False, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x12339cd30>, 'vec__use_idf': True, 'vec__vocabulary': None, 'cls__C': 1000, 'cls__class_weight': None, 'cls__dual': True, 'cls__fit_intercept': True, 'cls__intercept_scaling': 1, 'cls__loss': 'squared_hinge', 'cls__max_iter': 1000, 'cls__multi_class': 'ovr', 'cls__penalty': 'l2', 'cls__random_state': None, 'cls__tol': 0.0001, 'cls__verbose': 0}
