INFO:root:linear kernel: C = 0.001  f1 = 0.8611383828126288
INFO:root:{'memory': None, 'steps': [('vec', CountVectorizer(tokenizer=<function word_tokenize at 0x12dbdeb80>)), ('cls', SVC(C=1000, kernel='linear'))], 'verbose': False, 'vec': CountVectorizer(tokenizer=<function word_tokenize at 0x12dbdeb80>), 'cls': SVC(C=1000, kernel='linear'), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.int64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1, 'vec__ngram_range': (1, 1), 'vec__preprocessor': None, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x12dbdeb80>, 'vec__vocabulary': None, 'cls__C': 1000, 'cls__break_ties': False, 'cls__cache_size': 200, 'cls__class_weight': None, 'cls__coef0': 0.0, 'cls__decision_function_shape': 'ovr', 'cls__degree': 3, 'cls__gamma': 'scale', 'cls__kernel': 'linear', 'cls__max_iter': -1, 'cls__probability': False, 'cls__random_state': None, 'cls__shrinking': True, 'cls__tol': 0.001, 'cls__verbose': False}
INFO:root:linear kernel: C = 0.001  f1 = 0.8611383828126288
INFO:root:{'memory': None, 'steps': [('vec', CountVectorizer(tokenizer=<function word_tokenize at 0x12d15eb80>)), ('cls', SVC(C=1000, kernel='linear'))], 'verbose': False, 'vec': CountVectorizer(tokenizer=<function word_tokenize at 0x12d15eb80>), 'cls': SVC(C=1000, kernel='linear'), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.int64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1, 'vec__ngram_range': (1, 1), 'vec__preprocessor': None, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x12d15eb80>, 'vec__vocabulary': None, 'cls__C': 1000, 'cls__break_ties': False, 'cls__cache_size': 200, 'cls__class_weight': None, 'cls__coef0': 0.0, 'cls__decision_function_shape': 'ovr', 'cls__degree': 3, 'cls__gamma': 'scale', 'cls__kernel': 'linear', 'cls__max_iter': -1, 'cls__probability': False, 'cls__random_state': None, 'cls__shrinking': True, 'cls__tol': 0.001, 'cls__verbose': False}
INFO:root:linear kernel: C = 0.001  f1 = 0.8611383828126288
INFO:root:{'memory': None, 'steps': [('vec', CountVectorizer(tokenizer=<function word_tokenize at 0x130a1bc10>)), ('cls', SVC(C=1000, kernel='linear'))], 'verbose': False, 'vec': CountVectorizer(tokenizer=<function word_tokenize at 0x130a1bc10>), 'cls': SVC(C=1000, kernel='linear'), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.int64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1, 'vec__ngram_range': (1, 1), 'vec__preprocessor': None, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x130a1bc10>, 'vec__vocabulary': None, 'cls__C': 1000, 'cls__break_ties': False, 'cls__cache_size': 200, 'cls__class_weight': None, 'cls__coef0': 0.0, 'cls__decision_function_shape': 'ovr', 'cls__degree': 3, 'cls__gamma': 'scale', 'cls__kernel': 'linear', 'cls__max_iter': -1, 'cls__probability': False, 'cls__random_state': None, 'cls__shrinking': True, 'cls__tol': 0.001, 'cls__verbose': False}
INFO:root:linear kernel: C = 0.001  f1 = 0.8526940489827087
INFO:root:{'memory': None, 'steps': [('vec', CountVectorizer(tokenizer=<function word_tokenize at 0x12e3ddd30>)), ('cls', LinearSVC(C=1000))], 'verbose': False, 'vec': CountVectorizer(tokenizer=<function word_tokenize at 0x12e3ddd30>), 'cls': LinearSVC(C=1000), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.int64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1, 'vec__ngram_range': (1, 1), 'vec__preprocessor': None, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x12e3ddd30>, 'vec__vocabulary': None, 'cls__C': 1000, 'cls__class_weight': None, 'cls__dual': True, 'cls__fit_intercept': True, 'cls__intercept_scaling': 1, 'cls__loss': 'squared_hinge', 'cls__max_iter': 1000, 'cls__multi_class': 'ovr', 'cls__penalty': 'l2', 'cls__random_state': None, 'cls__tol': 0.0001, 'cls__verbose': 0}
INFO:root:linear kernel: C = 0.0001  f1 = 0.8604176321190359
INFO:root:{'memory': None, 'steps': [('vec', CountVectorizer(tokenizer=<function word_tokenize at 0x12da5dd30>)), ('cls', LinearSVC(C=1000))], 'verbose': False, 'vec': CountVectorizer(tokenizer=<function word_tokenize at 0x12da5dd30>), 'cls': LinearSVC(C=1000), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.int64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1, 'vec__ngram_range': (1, 1), 'vec__preprocessor': None, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x12da5dd30>, 'vec__vocabulary': None, 'cls__C': 1000, 'cls__class_weight': None, 'cls__dual': True, 'cls__fit_intercept': True, 'cls__intercept_scaling': 1, 'cls__loss': 'squared_hinge', 'cls__max_iter': 1000, 'cls__multi_class': 'ovr', 'cls__penalty': 'l2', 'cls__random_state': None, 'cls__tol': 0.0001, 'cls__verbose': 0}
INFO:root:linear kernel: C = 0.001  f1 = 0.940812694819055
INFO:root:{'memory': None, 'steps': [('vec', CountVectorizer(tokenizer=<function word_tokenize at 0x12b61ed30>)), ('cls', LinearSVC(C=1000))], 'verbose': False, 'vec': CountVectorizer(tokenizer=<function word_tokenize at 0x12b61ed30>), 'cls': LinearSVC(C=1000), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.int64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1, 'vec__ngram_range': (1, 1), 'vec__preprocessor': None, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x12b61ed30>, 'vec__vocabulary': None, 'cls__C': 1000, 'cls__class_weight': None, 'cls__dual': True, 'cls__fit_intercept': True, 'cls__intercept_scaling': 1, 'cls__loss': 'squared_hinge', 'cls__max_iter': 1000, 'cls__multi_class': 'ovr', 'cls__penalty': 'l2', 'cls__random_state': None, 'cls__tol': 0.0001, 'cls__verbose': 0}
INFO:root:linear kernel: C = 0.001  f1 = 0.940812694819055
INFO:root:{'memory': None, 'steps': [('vec', CountVectorizer(tokenizer=<function word_tokenize at 0x124c5ed30>)), ('cls', LinearSVC(C=0.001))], 'verbose': False, 'vec': CountVectorizer(tokenizer=<function word_tokenize at 0x124c5ed30>), 'cls': LinearSVC(C=0.001), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.int64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1, 'vec__ngram_range': (1, 1), 'vec__preprocessor': None, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x124c5ed30>, 'vec__vocabulary': None, 'cls__C': 0.001, 'cls__class_weight': None, 'cls__dual': True, 'cls__fit_intercept': True, 'cls__intercept_scaling': 1, 'cls__loss': 'squared_hinge', 'cls__max_iter': 1000, 'cls__multi_class': 'ovr', 'cls__penalty': 'l2', 'cls__random_state': None, 'cls__tol': 0.0001, 'cls__verbose': 0}
