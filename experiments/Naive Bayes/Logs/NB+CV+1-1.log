INFO:root:{'memory': None, 'steps': [('vec', CountVectorizer(tokenizer=<function word_tokenize at 0x7f2f56462550>)), ('cls', MultinomialNB())], 'verbose': False, 'vec': CountVectorizer(tokenizer=<function word_tokenize at 0x7f2f56462550>), 'cls': MultinomialNB(), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.int64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1, 'vec__ngram_range': (1, 1), 'vec__preprocessor': None, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x7f2f56462550>, 'vec__vocabulary': None, 'cls__alpha': 1.0, 'cls__class_prior': None, 'cls__fit_prior': True}
INFO:root:{'memory': None, 'steps': [('vec', CountVectorizer(tokenizer=<function word_tokenize at 0x7f413e995310>)), ('cls', MultinomialNB())], 'verbose': False, 'vec': CountVectorizer(tokenizer=<function word_tokenize at 0x7f413e995310>), 'cls': MultinomialNB(), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.int64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1, 'vec__ngram_range': (1, 1), 'vec__preprocessor': None, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x7f413e995310>, 'vec__vocabulary': None, 'cls__alpha': 1.0, 'cls__class_prior': None, 'cls__fit_prior': True}
INFO:root:{'memory': None, 'steps': [('vec', CountVectorizer(tokenizer=<function word_tokenize at 0x7f2da308e3a0>)), ('cls', MultinomialNB())], 'verbose': False, 'vec': CountVectorizer(tokenizer=<function word_tokenize at 0x7f2da308e3a0>), 'cls': MultinomialNB(), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.int64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1, 'vec__ngram_range': (1, 1), 'vec__preprocessor': None, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x7f2da308e3a0>, 'vec__vocabulary': None, 'cls__alpha': 1.0, 'cls__class_prior': None, 'cls__fit_prior': True}
