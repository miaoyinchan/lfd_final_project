INFO:root:{'memory': None, 'steps': [('vec', TfidfVectorizer(tokenizer=<function word_tokenize at 0x7fb8a74a7550>)), ('cls', MultinomialNB())], 'verbose': False, 'vec': TfidfVectorizer(tokenizer=<function word_tokenize at 0x7fb8a74a7550>), 'cls': MultinomialNB(), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.float64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1, 'vec__ngram_range': (1, 1), 'vec__norm': 'l2', 'vec__preprocessor': None, 'vec__smooth_idf': True, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__sublinear_tf': False, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x7fb8a74a7550>, 'vec__use_idf': True, 'vec__vocabulary': None, 'cls__alpha': 1.0, 'cls__class_prior': None, 'cls__fit_prior': True}
INFO:root:{'memory': None, 'steps': [('vec', TfidfVectorizer(tokenizer=<function word_tokenize at 0x7f858c580310>)), ('cls', MultinomialNB())], 'verbose': False, 'vec': TfidfVectorizer(tokenizer=<function word_tokenize at 0x7f858c580310>), 'cls': MultinomialNB(), 'vec__analyzer': 'word', 'vec__binary': False, 'vec__decode_error': 'strict', 'vec__dtype': <class 'numpy.float64'>, 'vec__encoding': 'utf-8', 'vec__input': 'content', 'vec__lowercase': True, 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1, 'vec__ngram_range': (1, 1), 'vec__norm': 'l2', 'vec__preprocessor': None, 'vec__smooth_idf': True, 'vec__stop_words': None, 'vec__strip_accents': None, 'vec__sublinear_tf': False, 'vec__token_pattern': '(?u)\\b\\w\\w+\\b', 'vec__tokenizer': <function word_tokenize at 0x7f858c580310>, 'vec__use_idf': True, 'vec__vocabulary': None, 'cls__alpha': 1.0, 'cls__class_prior': None, 'cls__fit_prior': True}
